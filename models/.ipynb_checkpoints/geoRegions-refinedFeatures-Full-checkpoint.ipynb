{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm as cm\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "import pickle\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhenjie/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:4: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "_train_=pd.read_csv('../input/train.csv')\n",
    "places=_train_['place_id'].unique()\n",
    "\n",
    "_train_.sort(['time'], inplace=True)\n",
    "trainRatio=0.7\n",
    "train=_train_[:int(len(_train_)*trainRatio)]\n",
    "train=train[train['accuracy']<1000]\n",
    "\n",
    "placeCounts=train['place_id'].value_counts()\n",
    "\n",
    "\n",
    "\n",
    "valid=_train_[int(len(_train_)*trainRatio):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def featureFactory(dataFrame):\n",
    "    print('Feature Augmentation')\n",
    "    minute = dataFrame.time%60\n",
    "    dataFrame['hour'] = dataFrame['time'].div(60).map(int)\n",
    "    dataFrame.drop(['time'], axis=1, inplace=True)\n",
    "    dataFrame['weekday'] = dataFrame['hour'].div(24).map(int)\n",
    "    dataFrame['month'] = dataFrame['weekday'].div(30).map(int)\n",
    "    dataFrame['year'] = (dataFrame['weekday'].div(365).map(int)+1)*10.0\n",
    "    dataFrame['hour'] = ((dataFrame.loc[:,'hour'].values%(24)+1)+minute.div(60.0))*(4.0)\n",
    "\n",
    "\n",
    "    dataFrame['weekday'] = (dataFrame['weekday']%7+1)*3.0\n",
    "    dataFrame['month'] = (dataFrame['month']%12+1)*2.0\n",
    "    dataFrame['accuracy'] = np.log10(dataFrame['accuracy'])*10.0\n",
    "    dataFrame['originalIndex'] = np.arange(len(dataFrame))\n",
    "\n",
    "    print ('processing is done')\n",
    "f=['x','y', 'accuracy','hour', 'weekday', 'month', 'year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def amplifier(dataFrame):    # add data for periodic time that hit the boundary\n",
    "    add_data1 = dataFrame[dataFrame.hour<6]\n",
    "    add_data1.hour = add_data1.hour+96\n",
    "\n",
    "    add_data2 = dataFrame[dataFrame.hour>98]\n",
    "    add_data2.hour = add_data2.hour-96\n",
    "    return add_data1.append(add_data2)\n",
    "    \n",
    "    print 'amp is done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Augmentation\n",
      "processing is done\n",
      "Feature Augmentation\n",
      "processing is done\n"
     ]
    }
   ],
   "source": [
    "featureFactory(train)\n",
    "train=train.append(amplifier(train))\n",
    "featureFactory(valid)\n",
    "# pd.options.mode.chained_assignment = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test=pd.read_csv('../input/test.csv')\n",
    "# featureFactory(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180.948864937\n"
     ]
    }
   ],
   "source": [
    "g=train.groupby('place_id')\n",
    "places=pd.DataFrame()\n",
    "places['xMean']=g['x'].mean()\n",
    "places['xStd']=g['x'].std()\n",
    "\n",
    "places['yMean']=g['y'].mean()\n",
    "places['yStd']=g['y'].std()\n",
    "\n",
    "sTime=time.time()\n",
    "\n",
    "train=train[(abs(train['y']-places.loc[train['place_id'], 'yMean'].values)<5*places.loc[train['place_id'], 'yStd'].values)\\\n",
    "         & (abs(train['x']-places.loc[train['place_id'], 'xMean'].values)<5*places.loc[train['place_id'], 'xStd'].values)\\\n",
    "           \n",
    "           ]\n",
    "\n",
    "print time.time()-sTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2 0 2 4 8\n"
     ]
    }
   ],
   "source": [
    "nXRegions=20\n",
    "nYRegions=40\n",
    "xMin=0\n",
    "xMax=2\n",
    "yMin=0\n",
    "yMax=2\n",
    "nXRegions=nXRegions*int(xMax-xMin)/10\n",
    "nYRegions=nYRegions*int(yMax-yMin)/10\n",
    "# train=train[(train['x']<xMax) & (train['x']>=xMin) & (train['y']<yMax) & (train['y']>=yMin)]\n",
    "# valid=valid[(valid['x']<xMax) & (valid['x']>=xMin) & (valid['y']<yMax) & (valid['y']>=yMin)]\n",
    "\n",
    "print xMin, xMax, yMin, yMax, nXRegions, nYRegions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def regionalForestPred(pred, classes, regionalPoints, data):\n",
    "\n",
    "    \n",
    "#     prob=[sorted(zip(p, classes))[-3:][::-1] for p in pred]\n",
    "    \n",
    "    \n",
    "#     regionalPred=[]\n",
    "#     for i in xrange(len(data)):\n",
    "#         sampleY=data['y'].iloc[i]\n",
    "#         sampleX=data['x'].iloc[i]\n",
    "#         sampleDay=data['day'].iloc[i]\n",
    "\n",
    "#         filteredPlaces=filter(lambda x: \\\n",
    "#                               sampleY<=yMaxMap[x[1]]+yAllowance and sampleY>=yMinMap[x[1]]-yAllowance and \\\n",
    "#                               sampleX<=xMaxMap[x[1]] and sampleX>=xMinMap[x[1]] and \\\n",
    "#                               sampleDay<=dayMaxMap[x[1]] and sampleDay>=dayMinMap[x[1]], prob[i])\n",
    "#         if filteredPlaces:\n",
    "#             filteredPlaces=map(lambda x: [x[0]*regionalPoints, x[1]], filteredPlaces)\n",
    "#         regionalPred.append(filteredPlaces)\n",
    "#     return regionalPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_test(modelSerialization=False, resultSerialization=False, model='rf', predictSet=valid, th=0, xMargin=0.05, yMargin=0.025):\n",
    "    \n",
    "    if modelSerialization or resultSerialization:\n",
    "        base='./'\n",
    "        folderName=model+'-'+time.strftime('%c')\n",
    "        folderPath=base+folderName+'/'\n",
    "        if not os.path.exists(folderPath):\n",
    "            os.makedirs(folderPath)\n",
    "            \n",
    "    \n",
    "#     f=['x','y','xTimeY','xDivY', 'accuracy', 'day', 'weekday', 'hourInDay','hourInDayShifted2', 'month', 'year']\n",
    "#     f=['x','y', 'accuracy', 'day','weekday', 'hourInDay','hourInDayShifted2', 'month', 'year']\n",
    "    f=['x','y', 'accuracy','hour', 'weekday', 'month', 'year']\n",
    "\n",
    "        \n",
    "    count=0\n",
    "    accuracies=[]\n",
    "    FBscores=[]\n",
    "    \n",
    "    startTime=time.time()\n",
    "    trainingTimes=[]\n",
    "    predictionTimes=[]\n",
    "    \n",
    "    testNum=0\n",
    "\n",
    "    roundNum=5\n",
    "\n",
    "    for xNum, yNum in ((xn, yn) for xn in xrange(nXRegions) for yn in xrange(nYRegions)):\n",
    "\n",
    "        xStep=round(1.0*(xMax-xMin)/nXRegions,roundNum)\n",
    "        xStart=round(xMin+xNum*xStep,roundNum)\n",
    "        xEnd=round(xStart+xStep,roundNum)\n",
    "\n",
    "        yStep=round(1.0*(yMax-yMin)/nYRegions,roundNum)\n",
    "        yStart=round(yMin+yNum*yStep,roundNum)\n",
    "        yEnd=round(yStart+yStep,roundNum)\n",
    "        \n",
    "#         regionalTrain=train[(train['x']<xEnd) & (train['x']>=xStart) & (train['y']<yEnd) & (train['y']>=yStart)]\n",
    "\n",
    "        regionalTrain=train[(train['x']<xEnd+xMargin) & (train['x']>=xStart-xMargin) & (train['y']<yEnd+yMargin) & (train['y']>=yStart-yMargin)]\n",
    "#         print xStart, xEnd, yStart, yEnd, len(regionalTrain)\n",
    "\n",
    "        \n",
    "            \n",
    "#         regionalTrain['sampleWeight']=\n",
    "\n",
    "\n",
    "        placesCounts=regionalTrain['place_id'].value_counts()\n",
    "        regionalTrain=regionalTrain[(placeCounts[regionalTrain['place_id'].values]>th).values]\n",
    "        \n",
    "#         tail=len(regionalTrain)/6\n",
    "#         latest=regionalTrain.sort_values(by=['time'])[:tail]\n",
    "#         regionalTrain=regionalTrain.append(latest)\n",
    "        \n",
    "        \n",
    "#         addition=regionalTrain[regionalTrain['hourInDay']<=4]\n",
    "#         addition['hourInDay']=addition['hourInDay']+24\n",
    "#         regionalTrain=regionalTrain.append(addition)\n",
    "        \n",
    "#         addition=regionalTrain[regionalTrain['hourInDay']>=20]\n",
    "#         addition['hourInDay']=addition['hourInDay']-24\n",
    "#         regionalTrain=regionalTrain.append(addition)\n",
    "\n",
    "\n",
    "        \n",
    "        if len(regionalTrain):\n",
    "#             regionalPredictSet=predictSet[(predictSet['x']<xEnd-margin) & (predictSet['x']>=xStart+margin) & (predictSet['y']<yEnd-margin) & (predictSet['y']>=yStart+margin)]\n",
    "            regionalPredictSet=predictSet[(predictSet['x']<xEnd) & (predictSet['x']>=xStart) & (predictSet['y']<yEnd) & (predictSet['y']>=yStart)]\n",
    "\n",
    "            testNum+=len(regionalPredictSet)\n",
    "\n",
    "            if model=='knn':\n",
    "                regionalTrain.loc[:,'x'] *= 500.0\n",
    "                regionalTrain.loc[:,'y'] *= 1000.0\n",
    "                regionalPredictSet.loc[:,'x'] *= 500.0\n",
    "                regionalPredictSet.loc[:,'y'] *= 1000.0\n",
    "#                 fWeights={'x': 500, 'y': 1000, 'accuracy':0, 'day': 0,'dayInMonth': 0, 'weekday': 3,\\\n",
    "#                           'hourInDay': 4, 'hourInDayShifted2': 0, 'month': 2, 'year': 10}\n",
    "#                 for ft in f:\n",
    "#                     regionalTrain[ft]=regionalTrain[ft].values*fWeights[ft]\n",
    "#                     regionalPredictSet[ft]=regionalPredictSet[ft].values*fWeights[ft]\n",
    "                \n",
    "            else:\n",
    "                fMean=regionalTrain[f].mean()\n",
    "                fStd=regionalTrain[f].std()\n",
    "\n",
    "                for ft in f:       \n",
    "                    regionalTrain[ft]=(regionalTrain[ft].values-fMean[ft])/(fStd[ft])\n",
    "                    regionalPredictSet[ft]=(regionalPredictSet[ft].values-fMean[ft])/(fStd[ft])\n",
    "#                 print regionalTrain['x'].describe()\n",
    "\n",
    "      \n",
    "            le=LabelEncoder()\n",
    "            regionalTrain['le']=le.fit_transform(regionalTrain['place_id'].values)\n",
    "\n",
    "#             s_w=1**(15-regionalTrain['day'].values/30.0)\n",
    "            \n",
    "            trainingStartTime=time.time()\n",
    "            if model=='rf':\n",
    "                s_w_rf=3**(-regionalTrain['day'].values)\n",
    "#                 s_w_rf=1.65-regionalTrain['day'].values\n",
    "    #             clf=RandomForestClassifier(n_jobs=-1, n_estimators=300, max_features=None).fit(regionalTrain[f], regionalTrain['le'])   \n",
    "            \n",
    "                clf=RandomForestClassifier(n_jobs=-1, n_estimators=150, random_state=0).fit(regionalTrain[f], regionalTrain['le'], sample_weight=s_w_rf)  \n",
    "\n",
    "            if model=='xgb':\n",
    "                clf=XGBClassifier(learning_rate=0.02, n_estimators=220, objective='multi:softprob', max_depth=3, seed=0).fit(regionalTrain[f], regionalTrain['le'])\n",
    "            \n",
    "            \n",
    "            if model=='knn':\n",
    "                clf=KNeighborsClassifier(n_neighbors=25, weights='distance',metric='manhattan', n_jobs=-1).fit(regionalTrain[f], regionalTrain['le'])\n",
    "         \n",
    "            trainingTimes.append(time.time()-trainingStartTime)\n",
    "            \n",
    "            \n",
    "            if modelSerialization:\n",
    "                modelFileName='{:04d}-{}-{}-{}-{}.clf'.format(count, xStart, xEnd, yStart, yEnd)\n",
    "                modelFilePath=folderPath+modelFileName\n",
    "                with open(modelFilePath, 'ab+') as fo:\n",
    "                    pickle.dump(clf, fo, pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "            \n",
    "            predictionStartTime=time.time()\n",
    "            prob=clf.predict_proba(regionalPredictSet[f])\n",
    "            predictionTimes.append(time.time()-predictionStartTime)\n",
    "            \n",
    "            pred=[sorted(zip(p, clf.classes_))[-3:][::-1] for p in prob]\n",
    "            prediction=le.inverse_transform([zip(*p)[1] for p in pred])\n",
    "            confidence=[zip(*p)[0] for p in pred]\n",
    "            \n",
    "            \n",
    "                \n",
    "            fbscoreForSerialization=-1\n",
    "            if 'place_id' in predictSet.columns:\n",
    "                regionalAccuracy=1.0*sum([regionalPredictSet['place_id'].iloc[i] in prediction[i] for i in xrange(len(regionalPredictSet))])/len(regionalPredictSet)\n",
    "                regionalConfidence=[sum(z)/len(regionalPredictSet) for z in zip(*confidence)]\n",
    "                regionalConfidence3=sum(regionalConfidence)\n",
    "\n",
    "                fbAccuracy=0\n",
    "                fbAccuracy+=1.0*sum([regionalPredictSet['place_id'].iloc[i] in prediction[i][:1] for i in xrange(len(regionalPredictSet))])/len(regionalPredictSet)\n",
    "                fbAccuracy+=1.0/2*sum([regionalPredictSet['place_id'].iloc[i] in prediction[i][1:2] for i in xrange(len(regionalPredictSet))])/len(regionalPredictSet)\n",
    "                fbAccuracy+=1.0/3*sum([regionalPredictSet['place_id'].iloc[i] in prediction[i][2:3] for i in xrange(len(regionalPredictSet))])/len(regionalPredictSet)\n",
    "                FBscores.append(fbAccuracy)\n",
    "                print 'region {}: {},{} accuracy: {},  fbAccu: {}, confidence: {}:'.format(count, xNum, yNum, regionalAccuracy, fbAccuracy, regionalConfidence3)\n",
    "                accuracies.append(regionalAccuracy)\n",
    "                if resultSerialization:\n",
    "                    fbscoreForSerialization=fbAccuracy\n",
    "                \n",
    "            if resultSerialization:\n",
    "                resultFileName='{:04d}-{}-{}-{}-{}.rst'.format(count, xStart, xEnd, yStart, yEnd)\n",
    "                resultFilePath=folderPath+resultFileName\n",
    "                predColumns=zip(*prediction)\n",
    "                confColumns=zip(*confidence)\n",
    "\n",
    "                \n",
    "                if len(predColumns[2])!=len(prediction):\n",
    "                    print 'missing values',count, len(predColumns[0])\n",
    "                \n",
    "                results=pd.DataFrame({'originalIndex': regionalPredictSet['originalIndex'].tolist(),\\\n",
    "                                      'x':regionalPredictSet['x'].tolist(), 'y':regionalPredictSet['y'].tolist(), \\\n",
    "                                      'accuracy':regionalPredictSet['accuracy'].tolist(), 'pred0':predColumns[0], \\\n",
    "                                      'pred1':predColumns[1], 'pred2':predColumns[2], 'conf0': confColumns[0], \\\n",
    "                                      'conf1': confColumns[1], 'conf2': confColumns[2], 'regionalFBScore': [fbscoreForSerialization]*len(regionalPredictSet)})\n",
    "\n",
    "                results.to_csv(resultFilePath)\n",
    "\n",
    "\n",
    "\n",
    "        count+=1\n",
    "        if count%10==0:\n",
    "\n",
    "            print '{} : total time {} s.'.format(count, time.time()-startTime)\n",
    "            print 'Average training Time: ', sum(trainingTimes)/len(trainingTimes)\n",
    "            print 'Average prediction Time: ', sum(predictionTimes)/len(predictionTimes)\n",
    "            print 'Average FB Score', np.mean(FBscores)\n",
    "            print \n",
    "            \n",
    "            startTime=time.time()\n",
    "            trainingTimes=[]\n",
    "            predictionTimes=[]\n",
    "\n",
    "\n",
    "    print \n",
    "    print np.mean(accuracies)\n",
    "    print np.var(accuracies)\n",
    "    print\n",
    "    print np.mean(FBscores)\n",
    "    print testNum\n",
    "    print 'done'\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "region 0: 0,0 accuracy: 0.680778032037,  fbAccu: 0.579915507833, confidence: 0.808461477913:\n",
      "region 1: 0,1 accuracy: 0.639189407395,  fbAccu: 0.537031056569, confidence: 0.789619850518:\n",
      "region 2: 0,2 accuracy: 0.631753859348,  fbAccu: 0.535752572899, confidence: 0.760886439019:\n",
      "region 3: 0,3 accuracy: 0.636774193548,  fbAccu: 0.539646697389, confidence: 0.798352838264:\n",
      "region 4: 0,4 accuracy: 0.65238480194,  fbAccu: 0.553609389503, confidence: 0.799855048745:\n",
      "region 5: 0,5 accuracy: 0.68078994614,  fbAccu: 0.573050069818, confidence: 0.788960360875:\n",
      "region 6: 0,6 accuracy: 0.647601780122,  fbAccu: 0.547250151091, confidence: 0.78165261081:\n",
      "region 7: 0,7 accuracy: 0.694501617171,  fbAccu: 0.590055212519, confidence: 0.80920183616:\n",
      "region 8: 1,0 accuracy: 0.635231788079,  fbAccu: 0.532744665195, confidence: 0.787102429596:\n",
      "region 9: 1,1 accuracy: 0.608250251532,  fbAccu: 0.503628159395, confidence: 0.751236527141:\n",
      "10 : total time 24.9809980392 s.\n",
      "Average training Time:  0.0160507440567\n",
      "Average prediction Time:  0.297830295563\n",
      "Average FB Score 0.549268348221\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-100-ced672feaaa7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodelSerialization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'knn'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresultSerialization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictSet\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxMargin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.06\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myMargin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.03\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-99-8b147fbbf430>\u001b[0m in \u001b[0;36mtrain_test\u001b[1;34m(modelSerialization, resultSerialization, model, predictSet, th, xMargin, yMargin)\u001b[0m\n\u001b[0;32m    140\u001b[0m                 \u001b[0mfbAccuracy\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mregionalPredictSet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'place_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mregionalPredictSet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mregionalPredictSet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m                 \u001b[0mfbAccuracy\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mregionalPredictSet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'place_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mregionalPredictSet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mregionalPredictSet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m                 \u001b[0mfbAccuracy\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mregionalPredictSet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'place_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mregionalPredictSet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mregionalPredictSet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m                 \u001b[0mFBscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfbAccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m                 \u001b[1;32mprint\u001b[0m \u001b[1;34m'region {}: {},{} accuracy: {},  fbAccu: {}, confidence: {}:'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxNum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myNum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregionalAccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfbAccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregionalConfidence3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/zhenjie/anaconda2/lib/python2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1284\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1285\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1286\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1288\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/zhenjie/anaconda2/lib/python2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m_getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1564\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1565\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1566\u001b[1;33m                 \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_scalar_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1567\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1568\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/zhenjie/anaconda2/lib/python2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m_convert_scalar_indexer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_convert_scalar_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m         \u001b[1;31m# if we are accessing via lowered dim, use the last dim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m         \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m         \u001b[1;31m# a scalar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_scalar_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_test(modelSerialization=False, model='knn', resultSerialization=False, predictSet=valid, th=0, xMargin=0.06, yMargin=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def makeSubmission(resultFolder):\n",
    "    results=[]\n",
    "    for f in os.listdir(resultFolder):\n",
    "        if f.endswith('.rst'):\n",
    "            fi=resultFolder+'/'+f\n",
    "            results.append(pd.read_csv(fi))\n",
    "    results=pd.concat(results)\n",
    "        \n",
    "    results.sort(['originalIndex'], inplace=True)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results=makeSubmission('../submissions/xgb-0.03-50Trees/')\n",
    "predictions=results[['pred0','pred1','pred2']]\n",
    "submit=pd.DataFrame()\n",
    "# submit.loc[:,'row_id']=np.arange(len(predictions))\n",
    "submit.loc[:,'place_id']=predictions[['pred0', 'pred1', 'pred2']].apply(lambda x: ' '.join([str(nx) for nx in x]), axis=1)\n",
    "submit.loc[:,'row_id']=np.arange(len(predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submit=submit[['row_id','place_id' ]]\n",
    "submit.set_index('row_id', inplace=True)\n",
    "submit.to_csv('../submissions/xgb0.03_50Trees.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p300=pd.read_csv('../submissions/rf300Trees10_250.csv').head()\n",
    "p50=pd.read_csv('../submissions/rf50Trees10_250.csv').head()\n",
    "p300\n",
    "p50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "others=pd.read_csv('../submissions/rf_submission_2016-06-03-19-57.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results=makeSubmission('rf-Tue Jun 14 18:18:14 2016')\n",
    "predictions=results[['pred0','pred1','pred2']].values\n",
    "\n",
    "realXMin=0\n",
    "realXMax=10.1\n",
    "realYMin=0\n",
    "realYMax=10.1\n",
    "\n",
    "real=_train_[int(len(_train_)*trainRatio):]\n",
    "\n",
    "real=real[(real['x']<realXMax) & (real['x']>=realXMin) & (real['y']<realYMax) & (real['y']>=realYMin)]\n",
    "real=real[['place_id']]\n",
    "\n",
    "print len(real)\n",
    "print len(predictions)\n",
    "\n",
    "# print len(predictions), len(real), results['originalIndex'].value_counts()\n",
    "\n",
    "totalHit=sum([real['place_id'].iloc[i] in predictions[i] for i in xrange(len(predictions))])\n",
    "score=0\n",
    "score+=1.0*sum([real['place_id'].iloc[i] in predictions[i][:1] for i in xrange(len(predictions))])\n",
    "score+=1.0/2*sum([real['place_id'].iloc[i] in predictions[i][1:2] for i in xrange(len(predictions))])\n",
    "score+=1.0/3*sum([real['place_id'].iloc[i] in predictions[i][2:3] for i in xrange(len(predictions))])\n",
    "\n",
    "print 1.0*totalHit/len(predictions)\n",
    "print score/len(predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "real.head()\n",
    "\n",
    "results[results['originalIndex']==934546]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r=pd.read_csv('rf-Sun Jun 12 12:17:04 2016/0000-0.0-1.0-0.0-0.04.rst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
